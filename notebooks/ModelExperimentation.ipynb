{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the following five simple baseline models to provide foundational insights. These will serve as the baseline performance metrics that any subsequent machine learning models aim to beat.\n",
    "\n",
    "1. **Global Mean Rating**:\n",
    "This model predicts the global mean rating for all user-item pairs, serving as the most basic form of recommendation.\n",
    "\n",
    "2. **User Mean Rating**:\n",
    "For this model, the mean rating of each user is calculated and used to predict ratings for all items the user has not yet interacted with.\n",
    "\n",
    "3. **Item Mean Rating**:\n",
    "In contrast to the User Mean Rating, this model focuses on the mean rating of each item and uses it to predict ratings for all users.\n",
    "\n",
    "4. **User-Item Mean Rating**:\n",
    "This model takes a more nuanced approach by predicting a rating for a user-item pair as the average of the user's mean rating and the item's mean rating. The formula is:\n",
    "$$prediction = \\frac{User Mean Rating + Item Mean Rating}{2}$$\n",
    "\n",
    "5. **Weighted Mean Ratings**:\n",
    "This model employs a weighted average of the user mean and item mean ratings. The weight ( w ) can be adjusted based on domain understanding. The formula is :\n",
    "$$prediction = w \\times User Mean Rating + (1 - w) \\times Item Mean Rating, \\space where \\space 0 \\leq w \\leq 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(\"../data/movies_metadata_after_eda.csv\")\n",
    "ratings_df = pd.read_csv(\"../data/ratings_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(ratings_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>global_mean_rating</th>\n",
       "      <th>user_mean_rating</th>\n",
       "      <th>item_mean_rating</th>\n",
       "      <th>user_item_mean_rating</th>\n",
       "      <th>weighted_mean_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>1028</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1049690908</td>\n",
       "      <td>3.540256</td>\n",
       "      <td>3.844444</td>\n",
       "      <td>3.836364</td>\n",
       "      <td>3.840404</td>\n",
       "      <td>3.841212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>665</td>\n",
       "      <td>4736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1010197684</td>\n",
       "      <td>3.540256</td>\n",
       "      <td>3.294304</td>\n",
       "      <td>3.540256</td>\n",
       "      <td>3.417280</td>\n",
       "      <td>3.392685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120</td>\n",
       "      <td>4002</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1167420604</td>\n",
       "      <td>3.540256</td>\n",
       "      <td>3.573684</td>\n",
       "      <td>3.318182</td>\n",
       "      <td>3.445933</td>\n",
       "      <td>3.471483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>257</td>\n",
       "      <td>1274</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1348544094</td>\n",
       "      <td>3.540256</td>\n",
       "      <td>3.801370</td>\n",
       "      <td>3.791667</td>\n",
       "      <td>3.796518</td>\n",
       "      <td>3.797489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>468</td>\n",
       "      <td>6440</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1296191715</td>\n",
       "      <td>3.540256</td>\n",
       "      <td>2.946196</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>3.423098</td>\n",
       "      <td>3.327717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp  global_mean_rating  user_mean_rating  \\\n",
       "0     128     1028     5.0  1049690908            3.540256          3.844444   \n",
       "1     665     4736     1.0  1010197684            3.540256          3.294304   \n",
       "2     120     4002     3.0  1167420604            3.540256          3.573684   \n",
       "3     257     1274     4.0  1348544094            3.540256          3.801370   \n",
       "4     468     6440     4.0  1296191715            3.540256          2.946196   \n",
       "\n",
       "   item_mean_rating  user_item_mean_rating  weighted_mean_rating  \n",
       "0          3.836364               3.840404              3.841212  \n",
       "1          3.540256               3.417280              3.392685  \n",
       "2          3.318182               3.445933              3.471483  \n",
       "3          3.791667               3.796518              3.797489  \n",
       "4          3.900000               3.423098              3.327717  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Sameer.ModelExperimentation import (\n",
    "    calculate_user_item_mean_rating,\n",
    "    calculate_weighted_mean_ratings,\n",
    "    calculate_rmse,\n",
    ")\n",
    "\n",
    "test_df = calculate_user_item_mean_rating(train_df, test_df)\n",
    "test_df, best_w, best_rmse = calculate_weighted_mean_ratings(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"constant_rating\"] = 2.5\n",
    "\n",
    "user_stats = train_df.groupby(\"userId\")[\"rating\"].agg([\"mean\", \"count\"]).reset_index()\n",
    "\n",
    "DAMPING_FACTOR = 0.2\n",
    "\n",
    "test_df = test_df.merge(user_stats, on=\"userId\", how=\"left\")\n",
    "test_df[\"damped_user_mean_rating\"] = ((1 - DAMPING_FACTOR) * test_df[\"mean\"]) + (\n",
    "    DAMPING_FACTOR * test_df[\"global_mean_rating\"][0]\n",
    ")\n",
    "\n",
    "rmse_damped_user_mean = calculate_rmse(\n",
    "    test_df[\"rating\"], test_df[\"damped_user_mean_rating\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_global_mean = calculate_rmse(test_df[\"rating\"], test_df[\"global_mean_rating\"])\n",
    "rmse_user_mean = calculate_rmse(test_df[\"rating\"], test_df[\"user_mean_rating\"])\n",
    "rmse_item_mean = calculate_rmse(test_df[\"rating\"], test_df[\"item_mean_rating\"])\n",
    "rmse_user_item_mean = calculate_rmse(\n",
    "    test_df[\"rating\"], test_df[\"user_item_mean_rating\"]\n",
    ")\n",
    "rmse_weighted_mean = calculate_rmse(test_df[\"rating\"], test_df[\"weighted_mean_rating\"])\n",
    "rmse_constant = calculate_rmse(test_df[\"rating\"], test_df[\"constant_rating\"])\n",
    "rmse_damped_user_mean = calculate_rmse(\n",
    "    test_df[\"rating\"], test_df[\"damped_user_mean_rating\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Global Mean Rating Model: 1.06\n",
      "RMSE for User Mean Rating Model: 0.96\n",
      "RMSE for Item Mean Rating Model: 1.00\n",
      "RMSE for User-Item Mean Rating Model: 0.92\n",
      "RMSE for Weighted Mean Rating Model: 0.92, with setting the best weight is 0.60\n",
      "RMSE for Constant Model: 1.49\n",
      "RMSE for Damped User-Mean Rating Model: 0.96\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE for Global Mean Rating Model: {rmse_global_mean:.2f}\")\n",
    "print(f\"RMSE for User Mean Rating Model: {rmse_user_mean:.2f}\")\n",
    "print(f\"RMSE for Item Mean Rating Model: {rmse_item_mean:.2f}\")\n",
    "print(f\"RMSE for User-Item Mean Rating Model: {rmse_user_item_mean:.2f}\")\n",
    "print(\n",
    "    f\"RMSE for Weighted Mean Rating Model: {best_rmse:.2f}, with setting the best weight is {best_w:.2f}\"\n",
    ")\n",
    "print(f\"RMSE for Constant Model: {rmse_constant:.2f}\")\n",
    "print(f\"RMSE for Damped User-Mean Rating Model: {rmse_damped_user_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building a movie recommendation system, we start with simple models due to it's importance in understanding how well we're doing. We found out that using both the average ratings from users and movies gets the best results, with a score (RMSE) of 0.92. This score shows how accurate our predictions are â€“ the lower, the better. Models that only look at users or movies are okay, but not as good. The worst model is the one that always guesses the same rating, with a score of 1.49. These simple models help us know what to beat; any new, fancier model should score lower than 0.92 to be better.\n",
    "\n",
    "We also look at more complex models because the simple ones don't consider what each user likes. Machine learning models can give more personalized suggestions, which is better for users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
